= Animation Stylization Collection
:imagesdir: asset
:tip-caption: üí°
:note-caption: üìù
:warning-caption: ‚ö†Ô∏è
:caution-caption: üîî
:important-caption: ‚ùó
:experimental:
:toc:

== Environment


[source,bash]
----
conda create -n torch python=3.8
conda activate torch
conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch
pip install pytorch-lightning==1.0.2 opencv-python matplotlib joblib scikit-image torchsummary webdataset albumentations
----

== Algorithm

=== https://github.com/TachibanaYoshino/AnimeGANv2[AnimeGAN: A Novel Lightweight GAN for Photo Animation]

[cols="^.^5,<.^95"]
|===

a| Setup 

a| 

. [yellow]#If you just want to use it, **just skip** the following step#

. download dataset from https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/dataset-1[here] and unzip

. download pretrain VGG19 from https://drive.google.com/file/d/1j0jDENjdwxCDb36meP6-u5xDBzmKBOjJ/view?usp=sharing[here] and unzip, then put it to `models/vgg19.npy`


|Train 

a|

* change **configs/animegan_pretrain.yaml** menu:dataset[root] to **your path**

* change **configs/animeganv2.yaml** menu:dataset[root] to **your path**

* pre-training generator 

[source,bash]
----
make train CODE=scripts/animegan_pretrain.py CFG=configs/animegan_pretrain.yaml
----

* training generator (use kbd:[Ctrl+c] can stop)

[source,bash]
----
make train CODE=scripts/animeganv2.py CFG=configs/animeganv2.yaml
----

* check progress 

[source,bash]
----
make tensorboard LOGDIR=logs/animeganv2/
----

|test 

a| 

. you can download my pretrained model from https://drive.google.com/drive/folders/1Bu5yIYBPGBlO4yNzUamhWdWs5o5gT1Rx?usp=sharing[here].

. run test command(process video coming soon)

[source,bash]
----
make infer CODE=scripts/animeganv2.py \
CKPT=logs/animeganv2/version_0/checkpoints/epoch=17.ckpt \
EXTRA=image_path:asset/animegan_test2.jpg
----

.2+| Result 

a| image::animegan_test2.jpg[animegantest,624,432,pdfwidth=50%,scaledwidth=50%]

a| image::animegan_test2_out.jpg[animegantestout,624,432,pdfwidth=50%,scaledwidth=50%]

|===

[NOTE]
====
. If you want to change style, you have run `scripts/animegan_datamean.py` get dataset mean difference, then change `configs/animeganv2.yaml` menu:dataset[data_mean].
====


=== https://github.com/SystemErrorWang/White-box-Cartoonization[Learning to Cartoonize Using White-box Cartoon Representations]

[cols="^.^5,<.^50,<.^50"]
|===

a| Setup 

2+a| 

. [yellow]#If you just want to use it, **just skip** the following step#

. download dataset from https://drive.google.com/file/d/10SGv_kbYhVLIC2hLlz2GBkHGAo0nec-3/view[here] and unzip

. download pretrain VGG19 from https://drive.google.com/file/d/1j0jDENjdwxCDb36meP6-u5xDBzmKBOjJ/view?usp=sharing[here] and unzip, then put it to `models/vgg19.npy`


|Train 

2+a|

* change **configs/whitebox_pretrain.yaml** menu:dataset[root] to **your path**

* change **configs/whitebox.yaml** menu:dataset[root] to **your path**

* pre-training generator 

[source,bash]
----
make train CODE=scripts/whiteboxgan_pretrain.py CFG=configs/whitebox_pretrain.yaml
----

* training generator (use kbd:[Ctrl+c] can stop)

[source,bash]
----
make train CODE=scripts/whiteboxgan.py CFG=configs/whitebox.yaml
----

* check progress 

[source,bash]
----
make tensorboard LOGDIR=logs/whitebox
----

|test 

2+a| 

. you can download my pretrained model from https://drive.google.com/drive/folders/1Bu5yIYBPGBlO4yNzUamhWdWs5o5gT1Rx?usp=sharing[here].

. run test command(process video coming soon)

[source,bash]
----
make infer CODE=scripts/whiteboxgan.py \
CKPT=logs/whitebox/version_0/checkpoints/epoch=4.ckpt \
EXTRA=image_path:asset/whitebox_test.jpg
----

.2+| Result 

a| image::whitebox_test.jpg[whiteboxtest,pdfwidth=50%,scaledwidth=50%]

a| image::whitebox_test_out.jpg[whiteboxtestout,pdfwidth=50%,scaledwidth=50%]

|===


[NOTE]
====
. The menu:model[superpixel_fn] has a great influence on the style. The `slic,sscolor` refer from offical code. defualt use `sscolor`.
====

== Repository structure

[%autowidth,cols="<.^,<.^"]
|===
| **Path** | **Description** 
| AnimeStylized | Repository root folder
| &boxvr;&nbsp; asset | Folder containing readme image assets
| &boxvr;&nbsp; <<anchor-configs,configs>> a| Folder containing configs defining model/data paramters, training hyperparamters. 
| &boxvr;&nbsp; <<anchor-datamodules,datamodules>> a| Folder with various dataset objects and transfroms.
| &boxvr;&nbsp; losses | Folder containing various loss functions for training, Only very general used loss functions are added here.

| &boxvr; <<anchor-models,models>> a| Folder containing all the models and training objects
| &boxvr;&nbsp; optimizers | Folder with common used optimizers
| &boxvr;&nbsp; <<anchor-scripts,scripts>> a| Folder with running scripts for training and inference
| &boxvr;&nbsp; utils | Folder with various utility functions
|===


[NOTE]
====
[[anchor-configs]]configs::
  * Each algorithm has a corresponding config file. 
  * Config file uses the YAML format

[[anchor-datamodules]]datamodules::
  * The `dataset.py`,`dsfunction.py`,`dstransform.py` contains common data module object's basic component
  * Basically, each algorithm has a corresponding `xxxds.py`

[[anchor-models]]models::
  * Basically, each algorithm has a corresponding `xxxnet.py`
  * Now, only have the gan architecture model, in future maybe add more.

[[anchor-scripts]]scripts::
  * Each algorithm has a corresponding `xxx.py`, implement the main training step and inference here
  * Each algorithm must add `run_common(xxxModule, xxxDataModule)` in main function, then you can use general trainer to training or testing


====

== Participation and Contribution

. Add custom `LightningDataModule` object as `xxxds.py` in `datamodules` dir.
. Add custom `Module` object model architecture as `xxxnet.py` in `networks` dir.
. Add custom `LightningDataModule` training script as `xxx.py` in `scripts` dir
. Add config file in `configs` dir, the paramters follow your custom `LightningModule` and `LightningDataModule`
. trianing your algorithm